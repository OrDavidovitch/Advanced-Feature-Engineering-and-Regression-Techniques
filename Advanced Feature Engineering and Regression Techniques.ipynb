{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdf1eba",
   "metadata": {},
   "source": [
    "First, we import the relevant libraries for the preprocessing section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd675e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64d22a",
   "metadata": {},
   "source": [
    "Reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1134049",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\ordav\\Desktop\\kaggle\\HousePrices')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a8f72",
   "metadata": {},
   "source": [
    "Getting some first impressions on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25041fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e32363",
   "metadata": {},
   "source": [
    "Our target variable is 'SalePrice'. We will seperate it from the explenatory variables.\n",
    "The Id is irrelevant as a feature, but we will need the test points' id's for the submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4499ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.SalePrice\n",
    "testId = test.Id\n",
    "train.drop('Id', axis = 1, inplace=True)\n",
    "test.drop('Id', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48fb7d",
   "metadata": {},
   "source": [
    "\n",
    "Looking at how nuch missing data we have. We will drop features with more than 20% missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e93758ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(train.index)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f743e591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(test.index)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "753d1ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FireplaceQu     690\n",
       "Fence          1179\n",
       "Alley          1369\n",
       "MiscFeature    1406\n",
       "PoolQC         1453\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()[train.isna().sum() > n * 0.2].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a91fd447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FireplaceQu     730\n",
       "Fence          1169\n",
       "Alley          1352\n",
       "MiscFeature    1408\n",
       "PoolQC         1456\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()[test.isna().sum() > m * 0.2].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a8b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDrop = ['FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC']\n",
    "train.drop(colsToDrop, axis = 1, inplace=True)\n",
    "test.drop(colsToDrop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a637dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToDrop = ['TotalBsmtSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageCars']\n",
    "train.drop(colsToDrop, axis = 1, inplace=True)\n",
    "test.drop(colsToDrop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d8f58",
   "metadata": {},
   "source": [
    "We have a large amount of categorical variables, and a lot of them has a high number of categories. We want to create a function that will transform them to ordinal variables. \n",
    "Lets describe the algorithm:\n",
    "\n",
    "1. Look at a categorical feature x.\n",
    "2. Look at each category in it {z1, z2, ... , zk}\n",
    "3. Seperate the 'SalePrice' by the category the observation belongs to. We have k numerical vectors.\n",
    "4. We compare each to vectors using Wilcoxon's rank sum test, to check for a significant difference. Since we have  up to (k Choose 2) seperate null hypothesis, we need to make a Bonfferoni correction to our significance level to keep our FWER as 0.05. We do not want to be too conservative, so we will look at it like making k tests.\n",
    "5. We now have s batches of categories. We consider each batch as equivalent (with respect to its effect on the target variable).\n",
    "6. We choose a representative category from each batch and compute the mean of 'SalePrice' in the category.\n",
    "7. We rank the batches by the mean we computed.\n",
    "8. An observation's new ordinal feature is the rank of the batch where its category is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58284b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def batches(x, alpha):\n",
    "    vals = pd.unique(train[x])\n",
    "    nUnique = len(vals)\n",
    "    to_compare = {}\n",
    "    for i in range(nUnique):\n",
    "        to_compare[vals[i]] = train.loc[train[x] == vals[i], 'SalePrice']\n",
    "    batches = {}\n",
    "    appended = []\n",
    "    for j in to_compare.keys():\n",
    "        if j not in appended:\n",
    "            batches[j] = [j]\n",
    "            appended.append(j)\n",
    "            for k in to_compare.keys():\n",
    "                if (k not in appended):\n",
    "                    t = scps.ranksums(x = to_compare[j], y = to_compare[k])\n",
    "                    p = t[1]\n",
    "                    if p >= (alpha / nUnique):\n",
    "                        batches[j].append(k)\n",
    "                        appended.append(k)\n",
    "    return batches\n",
    "\n",
    "def catToOrd(x, alpha):\n",
    "    d = batches(x, alpha)\n",
    "    e = []\n",
    "    train_new = pd.Series(0, index=np.arange(len(train.index)))\n",
    "    test_new = pd.Series(0, index=np.arange(len(test.index)))\n",
    "    for cat in d.keys():\n",
    "        mean_y = np.mean(train.loc[train[x] == cat, 'SalePrice'])\n",
    "        e.append((cat, mean_y))\n",
    "    e = sorted(e, key = itemgetter(1))\n",
    "    for i in range(len(train.index)):\n",
    "        filled = False\n",
    "        for j in range(len(e)):\n",
    "            if train[x][i] in d[e[j][0]]:\n",
    "                train_new[0 ,i] = j\n",
    "                filled = True\n",
    "        if not filled:\n",
    "            train_new[0, i] = np.nan\n",
    "    for i in range(len(test.index)):\n",
    "        filled = False\n",
    "        for j in range(len(e)):          \n",
    "            if test[x][i] in d[e[j][0]]:\n",
    "                test_new[0 ,i] = j\n",
    "                filled = True\n",
    "        if not filled:\n",
    "            test_new[0, i] = np.nan\n",
    "            \n",
    "    return [train_new, test_new]\n",
    "\n",
    "def catToOrdDF(alpha):\n",
    "    colsToDrop = []\n",
    "    types = train.dtypes\n",
    "    for j in range(len(types)):\n",
    "        if types[j] == object:\n",
    "            x = train.columns[j]\n",
    "            ordx = catToOrd(x, alpha)\n",
    "            train['New'+x] = ordx[0]\n",
    "            test['New'+x] = ordx[1]\n",
    "            colsToDrop.append(x)\n",
    "    train.drop(colsToDrop, axis = 1, inplace = True)\n",
    "    test.drop(colsToDrop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dbbc524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ordav\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:7784: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z = (s - expected) / np.sqrt(n1*n2*(n1+n2+1)/12.0)\n"
     ]
    }
   ],
   "source": [
    "catToOrdDF(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcc219",
   "metadata": {},
   "source": [
    "We notice that the 'MSSubClass' feature is also categorical, although it encoded with numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ff45466",
   "metadata": {},
   "outputs": [],
   "source": [
    "newCols = catToOrd('MSSubClass', 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c07d74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.MSSubClass = newCols[0]\n",
    "test.MSSubClass = newCols[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487466e4",
   "metadata": {},
   "source": [
    "We can now drop the 'SalePrice' column from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fab9876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('SalePrice', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70add7",
   "metadata": {},
   "source": [
    "We need to remember that our process might produce features that are constant .We check for features like that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "908644d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.std() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663a5da",
   "metadata": {},
   "source": [
    "We drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9c2bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolConst = (train.std() == 0)\n",
    "colsToDrop = boolConst.index[boolConst]\n",
    "train.drop(colsToDrop, axis = 1, inplace=True)\n",
    "test.drop(colsToDrop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183c34f",
   "metadata": {},
   "source": [
    "We want to avoid high correlation between features. We check which of them has a high linear correlation using Pearson's Correlation Coefficient, and drop ones who are closely realated to others. We drop the ones with a smaller Pearson's Correlation Coefficient to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "028d0b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCorrs(alpha):\n",
    "    corrs = train.corr()\n",
    "    corrCols = []\n",
    "    appearences = []\n",
    "    for i in range(len(corrs.index) - 1):\n",
    "        for j in range(i):\n",
    "            if abs(corrs).iloc[i,j] > alpha:\n",
    "                corrCols.append([corrs.columns[i], corrs.columns[j]])\n",
    "    colsToDrop = []\n",
    "    for k in corrCols:\n",
    "        if (k[0] not in colsToDrop) and (k[1] not in colsToDrop):\n",
    "            p0 = y_train.corr(train[k[0]])\n",
    "            p1 = y_train.corr(train[k[1]])\n",
    "            if p0 > p1:\n",
    "                colsToDrop.append(k[1])\n",
    "            else:\n",
    "                colsToDrop.append(k[0])\n",
    "    return(colsToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84344923",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsDrop = dropCorrs(0.75)\n",
    "train.drop(colsDrop, axis = 1, inplace = True)\n",
    "test.drop(colsDrop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e044ec0",
   "metadata": {},
   "source": [
    "We are measured using the RMSE between the log of the predictions and the log of the real values, and therefore we will use the log of 'SalePrice' for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6748bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_y_train = np.log(1 + y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d4293",
   "metadata": {},
   "source": [
    "We fill the missing data using Iterative Imputer, an imputation method that uses the other features and a regression model to fill in the missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ad26929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(max_iter=100)\n",
    "train = pd.DataFrame(imputer.fit_transform(train), columns=train.columns)\n",
    "test = pd.DataFrame(imputer.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0ae40",
   "metadata": {},
   "source": [
    "Scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd6115fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\n",
    "test = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08f8c3",
   "metadata": {},
   "source": [
    "We can now start and train some models. Tuning their parameters will be done using 5-fold CV.\n",
    "\n",
    "We first try to fit lasso to the data. We will also use this as feature selection method for different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae47e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha value: 0.003\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "paramGrid = {'alpha' : [i * 0.001 for i in range(1,100)]}\n",
    "lasso = Lasso(max_iter = 1000)\n",
    "gridSearch = GridSearchCV(estimator=lasso, param_grid=paramGrid)\n",
    "g = gridSearch.fit(train, log_y_train)\n",
    "print('Optimal alpha value: ' + str(g.best_params_['alpha']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69040b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = g.best_params_['alpha'], max_iter = 1000)\n",
    "lasso.fit(train, log_y_train)\n",
    "y_pred = lasso.predict(test)\n",
    "y_pred = np.exp(y_pred) - 1\n",
    "y_pred = pd.DataFrame({'Id' : testId, 'SalePrice' : y_pred})\n",
    "y_pred.to_csv('HousePricePredictionsLasso.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8aabf",
   "metadata": {},
   "source": [
    "We got a score of 0.13428. Lets see if we can improve. \n",
    "<br>\n",
    "We will try and use a random forest, with only the features that had a non-zero coefficient with lasso.\n",
    "<br>\n",
    "We need to tune the number of trees, the level of pruning and the number of features we choose from at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26f97001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced = train.loc[: , lasso.coef_ >  0]\n",
    "test_reduced = test.loc[: , lasso.coef_ >  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad938c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal ccp_alpha value: 0.0\n",
      "Optimal number of trees: 500\n",
      "Optimal number of features in each split: 0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "paramGrid = {'n_estimators' : [500, 1000], 'ccp_alpha' : [0.001 * i for i in range(4)],\n",
    "            'max_features' : [0.2, 0.3]}\n",
    "rFor = RandomForestRegressor()\n",
    "gridSearch = GridSearchCV(estimator=rFor, param_grid=paramGrid)\n",
    "g = gridSearch.fit(train_reduced, log_y_train)\n",
    "print('Optimal ccp_alpha value: ' + str(g.best_params_['ccp_alpha']) + '\\n' +\n",
    "     'Optimal number of trees: ' + str(g.best_params_['n_estimators']) + '\\n' +\n",
    "     'Optimal number of features in each split: ' + str(g.best_params_['max_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acaaf0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rFor = RandomForestRegressor(n_estimators = g.best_params_['n_estimators'],\n",
    "                             ccp_alpha = g.best_params_['ccp_alpha'],\n",
    "                            max_features = g.best_params_['max_features'])\n",
    "rFor.fit(train_reduced, log_y_train)\n",
    "y_pred = rFor.predict(test_reduced)\n",
    "y_pred = np.exp(y_pred) - 1\n",
    "y_pred = pd.DataFrame({'Id' : testId, 'SalePrice' : y_pred})\n",
    "y_pred.to_csv('HousePricePredictionsRandomForest.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603724f2",
   "metadata": {},
   "source": [
    "We got a score of 0.13572.\n",
    "This is small decrease in preformence than we got using Lasso. It implies that the linear model is not a bad idea.\n",
    "<br>\n",
    "Therefore, next we try and use a Support Vector Regressor with a linear kernel.\n",
    "<br>\n",
    "We need to try and find the optimal epsilon (allowed error) and C (inverse regularization coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6afd2137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal C value: 0.001\n",
      "Optimal epsilon value: 0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "linearSvr = SVR(kernel = 'linear')\n",
    "paramGrid = {'C' : [0.001 * i for i in range(1, 10)], 'epsilon' : [0.01 * i for i in range(6, 10)]}\n",
    "gridSearch = GridSearchCV(estimator=linearSvr, param_grid=paramGrid)\n",
    "g = gridSearch.fit(train_reduced, log_y_train)\n",
    "print('Optimal C value: ' + str(g.best_params_['C']) + '\\n' +\n",
    "     'Optimal epsilon value: ' + str(g.best_params_['epsilon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5d4c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSvr = SVR(kernel = 'linear', C=g.best_params_['C'], epsilon = g.best_params_['epsilon'])\n",
    "linearSvr.fit(train_reduced, log_y_train)\n",
    "y_pred = linearSvr.predict(test_reduced)\n",
    "y_pred = np.exp(y_pred) - 1\n",
    "y_pred = pd.DataFrame({'Id' : testId, 'SalePrice' : y_pred})\n",
    "y_pred.to_csv('HousePricePredictionsLinearSVR.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237606aa",
   "metadata": {},
   "source": [
    "We got a score of 0.13680, which is worse than both of the previous models.\n",
    "<br>\n",
    "We will therefore go back to tree based methods, specifically XGBoost.\n",
    "<br>\n",
    "We will adjust the learning rate and the lambda & alpha regularization parameters (l2 & l1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd1ab4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal learning rate: 0.04\n",
      "Optimal lambda: 4e-05\n",
      "Optimal alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb1 = xgb.XGBRegressor(n_estimators = 500, max_depth = 4)\n",
    "paramGrid = {'learning_rate' : [0.01 * i for i in range(1, 5)],\n",
    "            'reg_alpha' : [0.001 * i for i in range(1, 5)], 'reg_lambda' : [0.00001 * i for i in range(1, 5)]}\n",
    "gridSearch = GridSearchCV(estimator=xgb1, param_grid=paramGrid)\n",
    "g = gridSearch.fit(train_reduced, log_y_train)\n",
    "print('Optimal learning rate: ' + str(g.best_params_['learning_rate']) + '\\n' +\n",
    "     'Optimal lambda: ' + str(g.best_params_['reg_lambda']) + '\\n' +\n",
    "     'Optimal alpha: ' + str(g.best_params_['reg_alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c07099",
   "metadata": {},
   "source": [
    "In order to avoid overfitting we will add an early stopping criteria. If we had not made an improvement in the last 10 steps, we will stop the training and return only the ensemble we have built upn to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c972b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xgb1 = xgb.XGBRegressor(n_estimators = 500, max_depth = 4, \n",
    "                          learning_rate = g.best_params_['learning_rate'],\n",
    "                         reg_lambda = g.best_params_['reg_lambda'],\n",
    "                          reg_alpha = g.best_params_['reg_alpha'], early_stopping_rounds = 10)\n",
    "\n",
    "X_train_1, X_val, y_train_1, y_val = train_test_split(train_reduced, log_y_train, test_size = 0.1)\n",
    "xgb1.fit(X_train_1, y_train_1, eval_set = [(X_val, y_val)], verbose = 0)\n",
    "y_pred = xgb1.predict(test_reduced)\n",
    "y_pred = np.exp(y_pred) - 1\n",
    "y_pred = pd.DataFrame({'Id' : testId, 'SalePrice' : y_pred})\n",
    "y_pred.to_csv('HousePricePredictionsXGB.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67b9bd",
   "metadata": {},
   "source": [
    "We got a small improvement to our best score with 0.13214."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005a05b",
   "metadata": {},
   "source": [
    "Now we will try a new approach. We will combine all of the prior models into one using exponential weighting. This process is supposed to reduce prediction variance (and therefore avoid overfitting) with a slight increase in bias (compared to the best model).\n",
    "\n",
    "First, we need to train the model on a subset of the training data. We will use a random subset of 90%.\n",
    "The rest of the data will be used in order to determine the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33075d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_weights_1, y_train_1, y_weights_1 = train_test_split(train_reduced, log_y_train, test_size = 0.1)\n",
    "X_train_2, X_weights_2, y_train_2, y_weights_2 = train_test_split(train, log_y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89719dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 0.003, max_iter = 1000)\n",
    "lasso.fit(X_train_2, y_train_2)\n",
    "y_pred = lasso.predict(X_weights_2)\n",
    "e1 = np.exp(-np.linalg.norm(y_pred - y_weights_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e35d4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rFor = RandomForestRegressor(n_estimators = 500, max_features = 0.3, ccp_alpha=0)\n",
    "rFor.fit(X_train_1, y_train_1)\n",
    "y_pred = rFor.predict(X_weights_1)\n",
    "e2 = np.exp(-np.linalg.norm(y_pred - y_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "423b47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSvr = SVR(kernel = 'linear', C = 0.001, epsilon = 0.07)\n",
    "linearSvr.fit(X_train_1, y_train_1)\n",
    "y_pred = linearSvr.predict(X_weights_1)\n",
    "e3 = np.exp(-np.linalg.norm(y_pred - y_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efaaa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = xgb.XGBRegressor(n_estimators = 500, max_depth = 4, \n",
    "                          learning_rate = 0.04,\n",
    "                         reg_lambda = 4 * (10 ** -5),\n",
    "                          reg_alpha = 0.001, early_stopping_rounds = 10)\n",
    "\n",
    "X_train_2, X_val_1, y_train_2, y_val_1 = train_test_split(X_train_1, y_train_1, test_size = 0.1)\n",
    "xgb1.fit(X_train_2, y_train_2, eval_set = [(X_val_1, y_val_1)], verbose = 0)\n",
    "y_pred = xgb1.predict(X_weights_1)\n",
    "e4 = np.exp(-np.linalg.norm(y_pred - y_weights_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "898ee7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "den = e1 + e2 + e3 +  e4\n",
    "w1 = e1/den\n",
    "w2 = e2/den\n",
    "w3 = e3/den\n",
    "w4 = e4/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6a9fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lasso = lasso.predict(test)\n",
    "y_pred_rFor = rFor.predict(test_reduced)\n",
    "y_pred_svr = linearSvr.predict(test_reduced)\n",
    "y_pred_xgb = xgb1.predict(test_reduced)\n",
    "\n",
    "y_pred = w1 * y_pred_lasso + w2 * y_pred_rFor +  w3 * y_pred_svr + w4 * y_pred_xgb\n",
    "y_pred = np.exp(y_pred) - 1\n",
    "y_pred = pd.DataFrame({'Id' : testId, 'SalePrice' : y_pred})\n",
    "y_pred.to_csv('HousePricePredictionsAggregation.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1587aed",
   "metadata": {},
   "source": [
    "We indeed manage to get a better result than all the other models: 0.12725!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
